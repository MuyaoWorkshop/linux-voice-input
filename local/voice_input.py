#!/usr/bin/env python3
"""
è¯­éŸ³è¾“å…¥å·¥å…· - åŸºäº OpenAI Whisper
ä½¿ç”¨éº¦å…‹é£å½•éŸ³ï¼Œè½¬æ¢ä¸ºæ–‡å­—åå¤åˆ¶åˆ°å‰ªè´´æ¿
"""

import whisper
import pyaudio
import wave
import tempfile
import os
import subprocess
import sys
import time
import numpy as np

# å°è¯•å¯¼å…¥ OpenCC ç”¨äºç¹ç®€è½¬æ¢
try:
    from opencc import OpenCC
    OPENCC_AVAILABLE = True
except ImportError:
    OPENCC_AVAILABLE = False

# é…ç½®å‚æ•°
WHISPER_MODEL = "base"  # å¯é€‰: tiny, base, small, medium, large
LANGUAGE = "zh"         # ä¸­æ–‡è¯†åˆ«
SAMPLE_RATE = 16000     # é‡‡æ ·ç‡
CHANNELS = 1            # å•å£°é“
CHUNK = 1024            # éŸ³é¢‘å—å¤§å°
RECORD_SECONDS = 60     # æœ€é•¿å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰- å¯æ ¹æ®éœ€è¦è°ƒæ•´
SILENCE_THRESHOLD = 800 # é™éŸ³é˜ˆå€¼ï¼ˆåŸºäº int16 éŸ³é‡ï¼ŒèŒƒå›´ 0-32767ï¼‰
SILENCE_DURATION = 3.0  # é™éŸ³æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰åˆ¤å®šä¸ºç»“æŸ - é¿å…æ€è€ƒæ—¶è¢«æ‰“æ–­

class VoiceInput:
    def __init__(self):
        print("æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹...")
        self.model = whisper.load_model(WHISPER_MODEL)
        print(f"æ¨¡å‹åŠ è½½å®Œæˆ: {WHISPER_MODEL}")

        # åˆå§‹åŒ–ç¹ç®€è½¬æ¢å™¨
        if OPENCC_AVAILABLE:
            self.cc = OpenCC('t2s')  # ç¹ä½“è½¬ç®€ä½“
            print("ç¹ç®€è½¬æ¢: å·²å¯ç”¨")
        else:
            self.cc = None
            print("ç¹ç®€è½¬æ¢: æœªå¯ç”¨ (å¯é€‰å®‰è£…: pip install opencc-python-reimplemented)")

    def record_audio(self, filename):
        """å½•åˆ¶éŸ³é¢‘ï¼Œæ£€æµ‹é™éŸ³è‡ªåŠ¨åœæ­¢"""
        audio = pyaudio.PyAudio()

        # æ‰“å¼€éŸ³é¢‘æµ
        stream = audio.open(
            format=pyaudio.paInt16,
            channels=CHANNELS,
            rate=SAMPLE_RATE,
            input=True,
            frames_per_buffer=CHUNK
        )

        print(f"\nğŸ¤ å¼€å§‹å½•éŸ³... (è¯´è¯ååœé¡¿{SILENCE_DURATION}ç§’è‡ªåŠ¨ç»“æŸï¼Œæœ€é•¿{RECORD_SECONDS}ç§’)")

        frames = []
        silent_chunks = 0
        max_silent_chunks = int(SILENCE_DURATION * SAMPLE_RATE / CHUNK)
        started_speaking = False

        for i in range(0, int(SAMPLE_RATE / CHUNK * RECORD_SECONDS)):
            data = stream.read(CHUNK)
            frames.append(data)

            # æ­£ç¡®è®¡ç®—éŸ³é‡ï¼šå°†å­—èŠ‚æµè½¬æ¢ä¸º int16 æ•°ç»„
            audio_data = np.frombuffer(data, dtype=np.int16)
            volume = np.abs(audio_data).mean()

            # è°ƒè¯•è¾“å‡ºï¼ˆå¯é€‰ï¼‰ï¼šæ˜¾ç¤ºå®æ—¶éŸ³é‡
            # print(f"\rå½“å‰éŸ³é‡: {volume:.0f}", end="", flush=True)

            if volume > SILENCE_THRESHOLD:
                if not started_speaking:
                    print("\nâœ“ æ£€æµ‹åˆ°å£°éŸ³ï¼Œå¼€å§‹è®°å½•...")
                    started_speaking = True
                silent_chunks = 0
                print(".", end="", flush=True)
            elif started_speaking:
                silent_chunks += 1

            # å¦‚æœå·²ç»å¼€å§‹è¯´è¯ï¼Œä¸”é™éŸ³è¶…è¿‡é˜ˆå€¼ï¼Œåœæ­¢å½•éŸ³
            if started_speaking and silent_chunks > max_silent_chunks:
                print(f"\nâœ“ æ£€æµ‹åˆ° {SILENCE_DURATION} ç§’é™éŸ³ï¼Œåœæ­¢å½•éŸ³")
                break

        if not started_speaking:
            print("\nå½•éŸ³ç»“æŸï¼ˆæœªæ£€æµ‹åˆ°å£°éŸ³ï¼‰")
        else:
            print("\nå½•éŸ³ç»“æŸ")

        # åœæ­¢å¹¶å…³é—­æµ
        stream.stop_stream()
        stream.close()
        audio.terminate()

        # ä¿å­˜ä¸º WAV æ–‡ä»¶
        wf = wave.open(filename, 'wb')
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(audio.get_sample_size(pyaudio.paInt16))
        wf.setframerate(SAMPLE_RATE)
        wf.writeframes(b''.join(frames))
        wf.close()

    def transcribe(self, audio_file):
        """ä½¿ç”¨ Whisper è½¬å½•éŸ³é¢‘"""
        print("\nâ³ æ­£åœ¨è¯†åˆ«...")
        start_time = time.time()

        result = self.model.transcribe(
            audio_file,
            language=LANGUAGE,
            fp16=False  # CPU æ¨¡å¼å¿…é¡»è®¾ä¸º False
        )
        text = result["text"].strip()

        # ç¹ä½“è½¬ç®€ä½“
        if self.cc and text:
            text = self.cc.convert(text)

        elapsed = time.time() - start_time
        print(f"âœ“ è¯†åˆ«å®Œæˆ (è€—æ—¶ {elapsed:.1f} ç§’)")

        return text

    def copy_to_clipboard(self, text):
        """å°†æ–‡å­—å¤åˆ¶åˆ°å‰ªè´´æ¿"""
        if not text:
            print("æœªè¯†åˆ«åˆ°æ–‡å­—")
            return

        print(f"\nè¯†åˆ«ç»“æœ: {text}")

        try:
            # å°†æ–‡å­—å¤åˆ¶åˆ°å‰ªè´´æ¿
            process = subprocess.Popen(['xclip', '-selection', 'clipboard'],
                                      stdin=subprocess.PIPE)
            process.communicate(input=text.encode('utf-8'))
            print("\nâœ“ å·²å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼Œå¯ä½¿ç”¨ Ctrl+V ç²˜è´´")

        except FileNotFoundError:
            print("âŒ é”™è¯¯: æœªæ‰¾åˆ° xclip å‘½ä»¤ï¼Œè¯·å®‰è£…: sudo apt install xclip", file=sys.stderr)
        except Exception as e:
            print(f"âŒ å¤åˆ¶å¤±è´¥: {e}", file=sys.stderr)

    def run(self):
        """ä¸»æµç¨‹"""
        # åˆ›å»ºä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
            audio_file = tmp_file.name

        total_start = time.time()

        try:
            # 1. å½•éŸ³
            self.record_audio(audio_file)

            # 2. è¯†åˆ«
            text = self.transcribe(audio_file)

            # 3. å¤åˆ¶åˆ°å‰ªè´´æ¿
            self.copy_to_clipboard(text)

            # æ˜¾ç¤ºæ€»è€—æ—¶
            total_elapsed = time.time() - total_start
            print(f"\nâ±ï¸  æ€»è€—æ—¶: {total_elapsed:.1f} ç§’ (åŒ…æ‹¬ {SILENCE_DURATION} ç§’é™éŸ³æ£€æµ‹)")

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(audio_file):
                os.remove(audio_file)

def main():
    try:
        voice_input = VoiceInput()
        voice_input.run()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·å–æ¶ˆ")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()
